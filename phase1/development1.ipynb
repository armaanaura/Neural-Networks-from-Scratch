{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99a5a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feb0278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ab4a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a simple neurone class Neuron:\n",
    "class Neurone:\n",
    "    #constructor\n",
    "    def __init__ (self, num_inputs, activation = 'sigmoid'):\n",
    "        self.activation_type = activation\n",
    "        self.num_inputs = num_inputs\n",
    "        self.weights = np.random.uniform(-1,1,num_inputs)\n",
    "        self.bias = 0\n",
    "        self.last_input = None\n",
    "        self.last_pre_activation_output = None\n",
    "        self.last_output = None\n",
    "    \n",
    "    def calculate_weighted_sum(self, inputs):\n",
    "        weighted_sum = np.dot(self.weights, inputs)+self.bias\n",
    "        return weighted_sum\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.last_input = inputs.copy()\n",
    "        weighted_sum = self.calculate_weighted_sum(inputs)\n",
    "        self.last_pre_activation_output= weighted_sum\n",
    "        if self.activation_type == 'sigmoid':\n",
    "            last_output =  1/(1+np.exp(weighted_sum))\n",
    "        elif self.activation_type == 'relu':\n",
    "            last_output = max(0,weighted_sum)\n",
    "        elif self.activation_type == 'tanh':\n",
    "            last_output = np.tanh(weighted_sum)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Activation function '{self.activation_type}' is not available.\")\n",
    "        self.last_output = last_output\n",
    "        return last_output\n",
    "    \n",
    "    #this helps us to determine how much this neurone matters, if we change the raw output by changing the weights and inputs, will it affect its final output or it will be same? This is why we are finding derivative of the activation function wrt the raw output value. we need to find the rate of change of the function output with the raw output\n",
    "    def activation_derivative(self, output_before_activation):\n",
    "        if self.activation_type=='sigmoid':\n",
    "            output = 1/(1+np.exp(-output_before_activation))\n",
    "            return output * (1- output)\n",
    "        elif self.activation_type == 'relu':\n",
    "            return 1.0 if output_before_activation > 0 else 0.0\n",
    "        elif self.activation_type == 'tanh': \n",
    "            return 1- np.tanh(output_before_activation)**2\n",
    "        else:\n",
    "            raise NotImplementedError(f\"No derivative for '{self.activation_type}'\")\\\n",
    "                \n",
    "    def backward(self, delta, learning_rate):\n",
    "        upstream = self.weights * delta   # ∂L/∂x = w(old) * δ\n",
    "\n",
    "        gradient_weights = delta * self.last_input\n",
    "        gradient_bias = delta \n",
    "        self.weights -= learning_rate * gradient_weights\n",
    "        self.bias -= learning_rate * gradient_bias\n",
    "        \n",
    "        return upstream\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eafe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, num_neurones, num_inputs, activation_type = 'sigmoid'):\n",
    "        self.num_neurones = num_neurones\n",
    "        self.neurones : List[Neurone] = []\n",
    "        for _ in range(num_neurones):\n",
    "            neurone = Neurone(num_inputs,activation_type)\n",
    "            self.neurones.append(neurone)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        outputs = []\n",
    "        for neurone in self.neurones:\n",
    "            outputs.append(neurone.forward(inputs))\n",
    "        return outputs\n",
    "    \n",
    "    def backward(self, deltas, learning_rate):\n",
    "        \n",
    "        upstream_deltas = np.zeros(self.neurones[0].num_inputs)\n",
    "        \n",
    "        for delta, neurone in zip(deltas, self.neurones):\n",
    "            contribution = neurone.backward(delta, learning_rate)\n",
    "            upstream_deltas += contribution\n",
    "        return upstream_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cb7fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, num_inputs, loss_function_type='mean_squared_error'):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.loss_function_type = loss_function_type\n",
    "        self.layers: List[Layer] = []\n",
    "    \n",
    "    def add_layer(self, num_neurones, activation_type = 'sigmoid' ):\n",
    "        num_inputs = self.num_inputs\n",
    "        if not len(self.layers) == 0:\n",
    "            num_inputs = self.layers[-1].num_neurones\n",
    "        layer = Layer(num_neurones, num_inputs, activation_type)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        previous_output = inputs\n",
    "        for layer in self.layers:\n",
    "            previous_output = layer.forward(previous_output)\n",
    "        return np.array(previous_output)\n",
    "\n",
    "    def calculate_loss(self, y_true, y_pred):  #its cost function, not loss function\n",
    "        if self.loss_function_type == 'binary_corss_entropy':\n",
    "            return -np.mean((y_true*np.log(y_pred+1e-8))+((1-y_true)*np.log(1-y_pred+1e-8)))\n",
    "        elif self.loss_function_type == 'mean_squared_error':\n",
    "            return np.mean((y_true - y_pred)**2)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Loss '{self.loss_function_type}' not supported\")\n",
    "        \n",
    "    def train(self, inputs, y_true, learning_rate):\n",
    "        y_pred = self.forward(inputs)\n",
    "        \n",
    "        output_deltas = y_pred - y_true\n",
    "        \n",
    "        deltas = output_deltas\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            deltas = layer.backward(deltas, learning_rate)\n",
    "        \n",
    "        return self.calculate_loss(y_true, y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f45dba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.53944699 0.50707379]\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "\n",
    "my_nn = NeuralNetwork(3)\n",
    "my_nn.add_layer(4,activation_type='relu')\n",
    "my_nn.add_layer(2,activation_type='sigmoid')\n",
    "\n",
    "inputs = [0.9, -0.2, 0.1]\n",
    "output = my_nn.forward(inputs)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41eaf3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
